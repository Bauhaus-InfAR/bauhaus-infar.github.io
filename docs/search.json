[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to InfAR @ Bauhaus-Uni Weimar",
    "section": "",
    "text": "Notion\nMiro board\nDatasets"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/data_cleaning.html#descriptives",
    "href": "posts/data_cleaning.html#descriptives",
    "title": "Data cleaning",
    "section": "Descriptives",
    "text": "Descriptives\nThe Neufert 4.0 dataset comprised data from a total of N = 37,174 apartments.\n\n\n\nIn the first step, we excluded flats that were either underground or above 6th floor (n = 2,271), had a total are outside of the 25-200 m2 range (n = 1,094), lacked data on room noise (n = 4) or room sunlight (n = 4), or had a value of zero for either of these two variables (n = 810). As only a small number of flats (n = 387) had more than five rooms, these were also excluded. This resulted in a sample size of N = 32,990 apartments, whose characteristics are shown in Figure 1.\n\n\n\n\n\nFigure 1: Distributions of the main variables of interest after the initial data cleaning."
  },
  {
    "objectID": "posts/data_cleaning.html#shape-clusters",
    "href": "posts/data_cleaning.html#shape-clusters",
    "title": "Data cleaning",
    "section": "Shape clusters",
    "text": "Shape clusters\nTo identify clusters that contain apartments of with the same layout, we devised the following algorithm:\nFirst, to account for potential rotation of the apartments, we oriented them along the horizontal (x-axis) and the vertical (y-axis). This was done byt aligning the longest polygon edge within each apartments along the horizontal.\nNext, we converted the vector graphics representation of the apartments to 224×224 black and white raster representation that contained only area geometries (rooms, corridors, shafts, balconies etc. but not walls, columns, doors, or windows). Each such 224×224 matrix contained only values of 0 (black) and 1 (white).\nThese matrices were then used to compare to get a measure of distance between the apartment geometries. The apartments were first grouped according to number of rooms. Within each group, we then selected a random apartment as a reference matrix (\\(\\mathbf{A}\\)) and calculated the sum of squared difference between this reference and all other apartnent matrices (\\(\\mathbf{B}\\)):\n\\[SS = \\sum_{i = 1}^{n}\\sum_{j = 1}^{n}(\\mathbf{B}_{ij} - \\mathbf{A}_{ij})^2\\]\nwhere \\(n\\) = 224, the number of pixels per dimension.\nAfter the initial aligning of the floor plans along the x and y axes, there remained eight possible orientations each floor plan could have (see Figure 2). For this reason, each floor plan matrix was transformed into eight different matrices that reflected these rotattions and only then compared to the reference using the sum of squares measure. This resulted in eight distance values per floor plan, yielding an eight-dimensional difference space, with each floorplan representing a point in this space and the reference flat positioned at its origin.\n\n\n\n\n\nFigure 2: All eight possible orientations of a single aligned apartment representation\n\n\n\n\nIn order to further expand this difference space, we selected another floor plan as reference and repeated this process. This floor plan was chosen from the middle of the distance distributions in the first iteration. Thee resulting 16-dimensional data set was then passed to the Self-Organising Map algorithm (SOM, Kohonen…) with a grid size equal to \\(floor(\\sqrt{n})\\), where \\(n\\) represents the number of floor plans with a given number of rooms. This procedure yielded grid position for each of the analysed floor plans, clustering them according to shape.\nThe above-described algorithm produced a somewhat liberal clusters, occasionally grouping dissimilar floor plans into a single cluster and so a further step was implemented to refine the clustering. Within each of the SOM clusters, we started by designating the first (position was arbitrary) floor plan as the reference and once again calculated the distance between it and the eight orientations of all other floor plans in the same SOM cluster. If the smallest of the eight distance measures was less than an empirically determined threshold of 2,000, the floorplan in question was added to a cluster defined by the reference floor plan. Next, the first of the remaining floor plans that did not pass the threshold was designated a reference for the next cluster and the process repeated. Once all floor plans withing the same SOM cluster were assorted, the algorithm was reiterated on the next SOM cluster.\nWhile the “refinement” algorithm could have been applied to the raw, unclustered data, the benefit of first creating the SOM clusters was a massive reduction of apartment-to-apartment comparisons and thus of computational resources.\nThis procedure yieded 11,173 rotation- and translation-invariant clusters of geometrically identical floor plans. The number of floor plans per cluster ranged from 1 to 96 (see Figure 3).\n\n\n\n\n\nFigure 3: Histogram of the distribution of number of floorplans per cluster"
  },
  {
    "objectID": "posts/data_cleaning.html#filtering-observations",
    "href": "posts/data_cleaning.html#filtering-observations",
    "title": "Data cleaning",
    "section": "Filtering observations",
    "text": "Filtering observations\nOnce the data was sorted into clusters accodring to the geometry of floor plans, we removed apartments that were deemed as essentially duplicates with respect to the values of average room noise and average room sunlight. These essential duplicates were identified as follows:\nFirst, we discretised the noise variable into categories by 5 dB increments and the sunlight variable by increments of 200 lx. Within each cluste and for each level of noise, we then retained one observation at every given level of the discretised sunlight variable (if available). The retained observation was the one with the lowest score on the original, continuous average room sunlight variable.\nThis further reduced the data set to the final size of n = 20,419 essentially unique apartments."
  },
  {
    "objectID": "posts/data_cleaning/index.html#descriptives",
    "href": "posts/data_cleaning/index.html#descriptives",
    "title": "Data cleaning",
    "section": "Descriptives",
    "text": "Descriptives\nThe Neufert 4.0 dataset comprises data from a total of N = 37,174 apartments.\n\n\n\nIn the first step, we excluded flats that were either underground or above 6th floor (n = 2,271), had a total are outside of the 25-200 m2 range (n = 1,094), lacked data on room noise (n = 4) or room sunlight (n = 4), or had a value of zero for either of these two variables (n = 810). As only a small number of flats (n = 387) had more than five rooms, these were also excluded. This resulted in a sample size of N = 32,990 apartments, whose characteristics are shown in Figure 1.\n\n\n\n\n\nFigure 1: Distributions of the main variables of interest after the initial data cleaning."
  },
  {
    "objectID": "posts/data_cleaning/index.html#shape-clusters",
    "href": "posts/data_cleaning/index.html#shape-clusters",
    "title": "Data cleaning",
    "section": "Shape clusters",
    "text": "Shape clusters\nTo identify clusters that contain apartments of with the same layout, we devised the following algorithm:\nFirst, to account for potential rotation of the apartments, we oriented them along the horizontal (x-axis) and the vertical (y-axis). This was done byt aligning the longest polygon edge within each apartments along the horizontal.\nNext, we converted the vector graphics representation of the apartments to 224×224 black and white raster representation that contained only area geometries (rooms, corridors, shafts, balconies etc. but not walls, columns, doors, or windows). Each such 224×224 matrix contained only values of 0 (black) and 1 (white).\nThese matrices were then used to compare to get a measure of distance between the apartment geometries. The apartments were first grouped according to number of rooms. Within each group, we then selected a random apartment as a reference matrix (\\(\\mathbf{A}\\)) and calculated the sum of squared difference between this reference and all other apartment matrices (\\(\\mathbf{B}\\)):\n\\[SS = \\sum_{i = 1}^{n}\\sum_{j = 1}^{n}(\\mathbf{B}_{ij} - \\mathbf{A}_{ij})^2\\]\nwhere \\(n\\) = 224, the number of pixels per dimension.\nAfter the initial aligning of the floor plans along the x and y axes, there remained eight possible orientations each floor plan could have (see Figure 2). For this reason, each floor plan matrix was transformed into eight different matrices that reflected these rotations and only then compared to the reference using the sum of squares measure. This resulted in eight distance values per floor plan, yielding an eight-dimensional difference space, with each floor plan representing a point in this space and the reference flat positioned at its origin.\n\n\n\n\n\nFigure 2: All eight possible orientations of a single aligned apartment representation\n\n\n\n\nIn order to further expand this difference space, we selected another floor plan as reference and repeated this process. This floor plan was chosen from the middle of the distance distributions in the first iteration. Thee resulting 16-dimensional data set was then passed to the Self-Organising Map algorithm (SOM, Kohonen…) with a grid size equal to \\(floor(\\sqrt{n})\\), where \\(n\\) represents the number of floor plans with a given number of rooms. This procedure yielded grid position for each of the analysed floor plans, clustering them according to shape.\nThe above-described algorithm produced a somewhat liberal clusters, occasionally grouping dissimilar floor plans into a single cluster and so a further step was implemented to refine the clustering. Within each of the SOM clusters, we started by designating the first (position was arbitrary) floor plan as the reference and once again calculated the distance between it and the eight orientations of all other floor plans in the same SOM cluster. If the smallest of the eight distance measures was less than an empirically determined threshold of 2,000, the floor plan in question was added to a cluster defined by the reference floor plan. Next, the first of the remaining floor plans that did not pass the threshold was designated a reference for the next cluster and the process repeated. Once all floor plans within the same SOM cluster were assorted, the algorithm was reiterated on the next SOM cluster.\nWhile the “refinement” algorithm could have been applied to the raw, unclustered data, the benefit of first creating the SOM clusters was a massive reduction of apartment-to-apartment comparisons and thus of computational resources.\nThis procedure yielded 11,173 rotation- and translation-invariant clusters of geometrically identical floor plans. The number of floor plans per cluster ranged from 1 to 96 (see Figure 3).\n\n\n\n\n\nFigure 3: Histogram of the distribution of number of floorplans per cluster"
  },
  {
    "objectID": "posts/data_cleaning/index.html#filtering-observations",
    "href": "posts/data_cleaning/index.html#filtering-observations",
    "title": "Data cleaning",
    "section": "Filtering observations",
    "text": "Filtering observations\nOnce the data was sorted into clusters according to the geometry of floor plans, we removed apartments that were deemed as essentially duplicates with respect to the values of average room noise and average room sunlight. These essential duplicates were identified as follows:\nFirst, we discretised the noise variable into categories by 5 dB increments and the sunlight variable by increments of 200 lx. Within each cluster and for each level of noise, we then retained one observation at every given level of the discretised sunlight variable (if available). The retained observation was the one with the lowest score on the original, continuous average room sunlight variable.\nThis further reduced the data set to the final size of n = 20,419 essentially unique apartments."
  },
  {
    "objectID": "index.html#documents",
    "href": "index.html#documents",
    "title": "Welcome to InfAR @ Bauhaus-Uni Weimar",
    "section": "Documents",
    "text": "Documents"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Order By\n       Default\n         \n          File Name\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nFile Name\n\n\nModified\n\n\n\n\n\n\nall_flats_clustered.csv\n\n\n2/10/23, 12:04:09 PM\n\n\n\n\napartment_simulations.csv\n\n\n1/16/23, 2:52:50 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Datasets",
    "section": "",
    "text": "Order By\n       Default\n         \n          File Name\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nFile Name\n\n\nModified\n\n\n\n\n\n\nall_flats_clustered.csv\n\n\n10 Feb 2023, 12:04:09\n\n\n\n\napartment_simulations.csv\n\n\n16 Jan 2023, 14:52:50\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/shallow_nn/index.html",
    "href": "posts/shallow_nn/index.html",
    "title": "Shallow neural network exercise",
    "section": "",
    "text": "Let’s see if I can write a single hidden layer NN that would be able to say if an image of a floor plan shows a one-room apartment or not.\nFirst, load libraries.\nRead in the data:"
  },
  {
    "objectID": "posts/shallow_nn/index.html#data-processing",
    "href": "posts/shallow_nn/index.html#data-processing",
    "title": "Shallow neural network exercise",
    "section": "Data processing",
    "text": "Data processing\nTo make things simple, I’m going to only work with one apartment per shape cluster.\n\ndf.drop_duplicates(\"cluster\", keep = \"first\", inplace = True)\n\nI also only want to keep 2,000 one- and four-room apartments.\n\ndf = df[(df.room_number == 1) | (df.room_number == 4)]\ndf = df.head(2000)\n\nLet’s read in the floor plan images. These are saved locally on my computer, soz… ¯\\_(ツ)_/¯\nI’ll create an empty array and then append each image in a loop. cv2.imread(x, 0) reads the files in greyscale. I want to normalise the values so that they don’t range between 0 and 255 but 0-1. I also want to get rid of the greyscale by forcing non-white pixels to be black. That is what ... == 1 does and np.multiply(..., 1) casts the resulting boolean as integer.\n\nall_data = []\nfor i in range(0, df.shape[0]):\n    img = cv2.imread(\"../../../similarity/0\" + str(df.room_number.iloc[i]) + \"_room/\" + str(df.room_number.iloc[i]) + \"room_\" + df.bfa.iloc[i] + \".png\", 0)\n    all_data.append(np.multiply((img / 255) == 1, 1))\n\n# convert to NumPy array so that it has .shape\nall_data = np.array(all_data)\n\nLet’s see if that worked:\n\nplt.imshow(all_data[0], cmap = \"gray\")\n\n<matplotlib.image.AxesImage at 0x230f7b7fc40>\n\n\n\n\n\nNow, let’s flatten the images, reshaping each to a 1D array.\n\nall_data = all_data.reshape(all_data.shape[0], -1)\n\ndf = df[\"room_number\"].values.reshape(df.shape[0], 1) # only keep relevant columns\ndf = (df == 1).astype(int)\n\nAs a final step in data wrangling, I’m creating a 50-50 split of the data into train and test sets.\n\ntrain_ind = np.random.choice(range(0, df.shape[0]), 600, replace=False)\nmask = np.zeros(df.shape[0], dtype=bool)\nmask[train_ind] = True\nX_train = all_data[mask]\nY_train = df[mask]\nX_test = all_data[~mask]\nY_test = df[~mask]"
  },
  {
    "objectID": "posts/shallow_nn/index.html#model-design",
    "href": "posts/shallow_nn/index.html#model-design",
    "title": "Shallow neural network exercise",
    "section": "Model design",
    "text": "Model design\n\n\n\nStructure of the neural network. Stolen from Coursera."
  },
  {
    "objectID": "posts/shallow_nn/index.html#back-propagation",
    "href": "posts/shallow_nn/index.html#back-propagation",
    "title": "Shallow neural network exercise",
    "section": "Back propagation",
    "text": "Back propagation\nGiven \\(\\mathcal{J}\\) defined above, we need the following derivatives1 to implement the back propagation:\n\n\n\n\n\n\nNote\n\n\n\n\\(\\circ\\) denotes element-wise product.\n\n\n\\[\n\\begin{aligned}\n\\mathcal{J}^\\prime_{Z^{[2]}} &= A^{[2]} - Y \\\\ &= dZ^{[2]}\\\\\n\\mathcal{J}^\\prime_{W^{[2]}} &= \\frac{1}{m}\\mathbf{A}^{[1]\\textsf{T}}dZ^{[2]} \\\\\n\\mathcal{J}^\\prime_{b^{[2]}} &= \\frac{1}{m}\\sum_{i=1}^{m}dZ_i^{[2]} \\\\\n\\mathcal{J}^\\prime_{\\mathbf{Z}^{[1]}} &= dZ^{[2]}\\mathbf{W}^{[2]\\textsf{T}} \\circ g^\\prime(\\mathbf{Z}^{[1]}) \\\\\n&= dZ^{[2]}\\mathbf{W}^{[2]\\textsf{T}} \\circ (1 - \\mathbf{A}^{[1]}\\circ\\mathbf{A}^{[1]}) \\\\\n&= dZ^{[1]} \\\\\n\\mathcal{J}^\\prime_{\\mathbf{W}^{[1]}} &= \\frac{1}{m}\\mathbf{X}^\\textsf{T}dZ^{[1]} \\\\\n\\mathcal{J}^\\prime_{B^{[1]}} &= \\frac{1}{m}\\sum_{i=1}^{m}dZ_i^{[1]} \\\\\n\\end{aligned}\n\\]\n\\(g^\\prime(\\mathbf{Z}^{[1]})\\) is the derivative of the activation (link) function in the hidden layer, in this case the \\(\\tanh\\) function. \\(g(\\mathbf{Z}^{[1]}) = \\tanh(\\mathbf{Z}^{[1]}) = \\mathbf{A}^{[1]}\\) and its derivative is \\(1 - \\mathbf{A}^{[1]}\\circ\\mathbf{A}^{[1]}\\).\n\nBP implementation\n\ndef backward(X, Y, params, fwd_steps):\n    m = X.shape[0]\n    A1 = fwd_steps[\"A1\"]\n    A2 = fwd_steps[\"A2\"]\n    W1 = params[\"W1\"]\n    W2 = params[\"W2\"]\n    b1 = params[\"b1\"]\n    b2 = params[\"b2\"]\n\n    dZ2 = A2 - Y\n    dW2 = np.dot(A1.T, dZ2)/m\n    db2 = np.mean(dZ2)\n    dZ1 = np.dot(dZ2, W2.T) * (1 - A1 ** 2)\n    dW1 = np.dot(X.T, dZ1)/m\n    db1 = np.mean(dZ1)\n\n    return {\n        \"dW1\": dW1,\n        \"db1\" : db1,\n        \"dW2\": dW2,\n        \"db2\" : db2,\n    }\n\nOn top of the back propagation function, we also need a function that updates parameters. The one below modifies the object passed to params= in place and doesn’t return anything.\n\ndef update_params(params, grads, learning_rate):\n    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n    params[\"b1\"] -= learning_rate * grads[\"db2\"]\n    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n\n    return"
  },
  {
    "objectID": "posts/shallow_nn/index.html#forward-propagation",
    "href": "posts/shallow_nn/index.html#forward-propagation",
    "title": "Shallow neural network exercise",
    "section": "Forward propagation",
    "text": "Forward propagation\n\n\n\n\n\n\nNote\n\n\n\nI’m following a linear regression convention below as that’s the one I’m most familiar with. In ML literature, you may find the same equation expressed as \\(\\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]\\textsf{T}}\\mathbf{X} + b\\). This is really the same thing and the difference is only due to the fact that the \\(\\mathbf{Z}^{[1]}\\) and \\(\\mathbf{W}^{[1]}\\) matrices here are transposed with respect to those in ML literature.\n\n\n\\[\\mathbf{Z}^{[1]} = \\mathbf{X}\\mathbf{W}^{[1]} + B^{[1]},\\]\nwhere\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n    x_{11} & \\dotsb & x_{1n}\\\\\n    x_{21} & \\dotsb & x_{2n}\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    x_{m1} & \\dotsb & x_{mn}\n\\end{bmatrix},\n\\]\nis the \\(m\\times n\\) matrix of \\(n\\) features for \\(m\\) training examples (observations).\n\\[\n\\mathbf{W}^{[1]} = \\begin{bmatrix}\n    w_{11} & w_{12} & w_{13} & w_{14}\\\\\n    w_{21} & w_{22} & w_{23} & w_{24}\\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n    w_{n1} & w_{n2} & w_{n3} & w_{n4}\n\\end{bmatrix},\n\\]\nis a \\(n\\times 4\\) matrix of weights, with each column representing the weights for one of the four nodes of the hidden layer. Rows contain the regression weights for all of the \\(n\\) features.\n\\[\nB^{[1]} = \\begin{bmatrix}\n    b_1 \\\\\n    b_2 \\\\\n    b_3 \\\\\n    b_4\n\\end{bmatrix}\n\\]\nis a column vector of biases/intercepts.\nThe resulting matrix \\(\\mathbf{Z}^{[1]}\\) is a \\(m\\times 4\\) matrix of outcomes of this linear regression with one column per node of the hidden layer.\n\\[\n\\begin{aligned}\n\\mathbf{A}^{[1]} &= \\tanh(\\mathbf{Z}^{[1]}) \\\\\nZ^{[2]} &= \\mathbf{A}^{[1]}W^{[2]} + b^{[2]} \\\\\n\\hat{Y} &= Z^{[2]} = \\sigma(Z^{[2]})\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\ny^{(i)}_{pred} &= \\begin{cases}\n    1 & \\text{if }\\hat{y}^{(i)} > 0.5 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n\\]\nIn the above, \\(\\hat{Y}\\) is a column vector of \\(m\\) predicted probabilities, one for each training example. \\(\\hat{y}^{(i)}\\) and \\(y^{(i)}_{pred}\\) are the predicted probability and the categorical prediction for the \\(i\\)th example, respectively.\n\n\n\n\n\n\nNote\n\n\n\nSince this network has only 2 layers, \\(W^{[2]}\\), \\(Z^{[2]}\\) and \\(A^{[2]}\\) are column vectors, hence the italics.\n\n\n\nFP implementation\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    return (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)\n\ndef model_dimensions(data, outcome, n_nodes = 4):\n    # n = number of predictor features\n    # m = n of observations\n    # y_size = n of outcome features\n    n = data.shape[1]\n    m = data.shape[0]\n    y_size = outcome.shape[1]\n\n    return n, m, y_size, n_nodes\n\ndef params_init(n_x_features, n_y_features, n_nodes):\n    W1 = np.random.randn(n_x_features, n_nodes) * .001\n    b1 = np.zeros(n_nodes).reshape(n_nodes, 1)\n    W2 = np.random.randn(n_nodes, 1) * .001\n    b2 = 0\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\ndef forward(x, params):\n    W1 = params[\"W1\"]\n    b1 = params[\"b1\"]\n    W2 = params[\"W2\"]\n    b2 = params[\"b2\"]\n    \n    Z1 = np.dot(x, params[\"W1\"]) + params[\"b1\"].T\n    A1 = tanh(Z1)\n    Z2 = np.dot(A1, W2) + b2\n    A2 = sigmoid(Z2)\n\n    steps = {\n        \"Z1\": Z1,\n        \"A1\": A1,\n        \"Z2\": Z2,\n        \"A2\": A2\n    }\n\n    return A2, steps"
  },
  {
    "objectID": "posts/shallow_nn/index.html#cost-function",
    "href": "posts/shallow_nn/index.html#cost-function",
    "title": "Shallow neural network exercise",
    "section": "Cost function",
    "text": "Cost function\nThe loss function of this model for a single example is:\n\\[\n\\mathcal{L}(\\mathbf{W}^{[1]}, B^{[1]}, W^{[2]}, b^{[2]}) = y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{y}^{(i)}),\n\\]\nwith the model cost function being the average of all \\(\\mathcal{L}\\), for each example:\n\\[\n\\mathcal{J} = -\\frac{1}{2}\\sum_{i=0}^{m}\\mathcal{L}_i\n\\]\n\nCost implementation\n\ndef cost(predicted, observed):\n    yhat = predicted\n    m = yhat.shape[0]\n    y = observed\n\n    cost = -np.mean(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n\n    return float(np.squeeze(cost))"
  },
  {
    "objectID": "posts/shallow_nn/index.html#lets-run-it",
    "href": "posts/shallow_nn/index.html#lets-run-it",
    "title": "Shallow neural network exercise",
    "section": "Let’s run it",
    "text": "Let’s run it\n\nm1 = nn_model(X_train, Y_train, 10)\n\nCost after iteration 0: 0.693140\n\n\nModel converged after 9 iterations"
  },
  {
    "objectID": "posts/shallow_nn/index.html#predict",
    "href": "posts/shallow_nn/index.html#predict",
    "title": "Shallow neural network exercise",
    "section": "Predict",
    "text": "Predict\n\ndef predict(X, params):\n    yhat, steps = forward(X, params)\n    return (yhat > .5).astype(int)\n\n\npred = predict(X_test, m1[\"params\"])\n\n\n\nOur simple model correctly classified 64.9% of the floor plans in the test set"
  },
  {
    "objectID": "posts/shallow_nn/index.html#full-model",
    "href": "posts/shallow_nn/index.html#full-model",
    "title": "Shallow neural network exercise",
    "section": "Full model",
    "text": "Full model\nOK, time to put it all together. I’m also including a tolerance parameter. If the absolute difference between the costs of two consecutive iterations, the model is deemed to have converged and the loop breaks.\n\ndef nn_model(X, Y, n_nodes, learning_rate=.2, n_iter=10000, tol=1e-5, seed = None, print_cost=True):\n    start = time.time() # start timer\n    if seed is not None:\n        np.random.seed(seed)\n\n    J = []\n    n_x, n_samp, n_y, foo = model_dimensions(X, Y, n_nodes)\n    gradients = {\n        \"dW1\": np.zeros(n_x * n_nodes).reshape(n_x, n_nodes),\n        \"db1\" : np.zeros(n_nodes).reshape(n_nodes, 1),\n        \"dW2\": np.zeros(n_nodes).reshape(n_nodes, 1),\n        \"db2\" : 0,\n    }\n    params = params_init(n_x, n_y, n_nodes)\n\n    for i in range(0, n_iter + 1):\n        if i == n_iter:\n            print(\"Model failed to converge\")\n            break\n        Y_hat, steps = forward(X, params)\n        J.append(cost(Y_hat, Y))\n        if i > 0 and abs(J[i - 1] - J[i]) < tol:\n            end = time.time()\n            elapsed = end - start\n            print (f'Model converged in {round(elapsed, 1)} seconds after {i + 1} iterations')\n            break\n        gradients = backward(X, Y, params, steps)\n        update_params(params, gradients, learning_rate)\n\n        if print_cost and i % 50 == 0:\n            print (f'Cost after iteration {i}: {J[i]}')\n        \n\n    return {\"params\": params, \"cost\": J}\n\nCool, all that’s left to do is run the model on the training set:\n\nm1 = nn_model(X_train, Y_train, 10)\n\nCost after iteration 0: 0.6930327112203966\n\n\nModel converged in 1.7 seconds after 10 iterations\n\n\nThat was actually way faster than I would have expected…"
  },
  {
    "objectID": "posts/shallow_nn/index.html#testing-model",
    "href": "posts/shallow_nn/index.html#testing-model",
    "title": "Shallow neural network exercise",
    "section": "Testing model",
    "text": "Testing model\nFirst, a quick wrapper function that returns a binary array of predictions: 0 = not a one-room floor plan; 1 = one-room floor plan.\n\ndef predict(X, params):\n    yhat, steps = forward(X, params)\n    return (yhat > .5).astype(int)\n\nNext, we need to use the previously learnt parameters stored in m1 and pass the test dataset to predict().\n\npred = predict(X_test, m1[\"params\"])\n\nFinally, let’s calculate the classification accuracy:\n\nacc = float((np.dot(Y_test.T, pred) + np.dot(1 - Y_test.T, 1 - pred)) / float(Y_test.size) * 100)\n\n\n\nOur simple model correctly classified 63.7% of the floor plans in the test set\n\n\nNot amazing but given that the model only uses raw pixel values of the images, it’s not that bad either."
  }
]