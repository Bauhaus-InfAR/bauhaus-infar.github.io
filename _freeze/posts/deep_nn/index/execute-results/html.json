{
  "hash": "b5c0b689ff4a14c97e5a141437c4ddbe",
  "result": {
    "markdown": "---\ntitle: \"Going deeper\"\nauthor: Milan\ndate: \"2/21/2023\"\ndate-format: \"D MMM YYYY\"\ncategories: [Neufert 4.0, machine learning]\nimage: \"https://www.slashfilm.com/img/gallery/the-deep-house-release-date-cast-and-more/intro-1634160218.webp\"\nexecute:\n  warning: false\n  message: false\ndraft: true\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nIn this post, I follow up on the [previous exercise](../shallow_nn/) and turn the shallow NN into a deep one by expanding the number of hidden layers to and arbitrary number.\n\nI am using the same data as before, following the same cleaning procedure.\n\n## Deep NN with L layers\n\n<!-- A lesson learnt from building the shallow NN is that I should really stick to the way the linear algebra is conceptualised in ML literature, rather than sticking to what I find easier to wrap my head around -->\n\n## Parameter shapes\n\nGoing back to the shallow NN, there were a few matrices/vectors in the forward propagation:\n\n- $\\mathbf{X}$: a $m\\times j$ matrix of $j$ features/variables per $m$ examples/observations\n- $\\mathbf{W^{[1]}}$: a $n^{[1]}\\times j$ matrix, where $n^{[1]}$ is the number of nodes in the 1^st^ layer ($L^{[1]}$)\n- $b^{[1]}$: a $n^{[1]}$-dimensional column vector of biases/intercepts of $L^{[1]}$\n- a $\\mathbf{Z^{[1]}}$: a $m\\times n^{[1]}$ matrix of activations (regression outputs), where $\\mathbf{Z^{[1]}} = \\mathbf{X}\\mathbf{W^{[1]}}+b^{[1]}$\n- $\\mathbf{A^{[1]}}$: also a $m\\times n^{[1]}$ matrix of $L^{[1]}$ outputs after applying the activation function $g^{[1]}$, $\\mathbf{A^{[1]}} = g^{[1]}(\\mathbf{Z^{[1]}})$\n- $\\mathbf{W^{[2]}}$: a $m$-dimensional column vector, since there is only a single node in the output layer\n- $b^{[2]}$: a scalar bias for the output layer.\n- $\\mathbf{Z^{[2]}}$: a $m$-dimensional column vector of output layer activations, $\\mathbf{Z^{[2]}} = \\mathbf{A^{[2]}}\\mathbf{W^{[2]}}+b^{[2]}$\n- $\\mathbf{A^{[2]}} = \\hat{y}$: also a $m$-dimensional column vector of model outputs after applying the activation function $g^{[2]}$, $\\mathbf{A^{[2]}} = g^{[2]}(\\mathbf{Z^{[2]}})$\n\nThe above can be generalised for any layer $l$ of a deep NN with $L$ layers ($l=0$ being the input layer) as:\n\n| Parameter |  Shape  |\n|-----------|---------|\n| $\\mathbf{Z^{[l]}}, \\mathbf{A^{[l]}}$ | $(m, n^{[l]})$ |\n| $\\mathbf{W^{[l]}}$ | $(n^{[l-1]}, n^{[l]})$ |\n| $b^{[l]}$ | $(n^{[1]}, 1)$ |\n\nNote that $\\mathbf{A^{[0]}} = \\mathbf{X}$, whereby $n^{[0]}$ is the number of features. There is no $\\mathbf{Z^{[0]}}$, $\\mathbf{W^{[0]}}$, or $b^{[0]}$.\n\n<!-- In our model, $n^{[L]} = 1$. -->\n\n### Implemetation\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef init_params(data: np.ndarray, L: int, n_nodes: list[int]) -> list[dict]:\n    # n = number of nodes in layer L_i\n    # n[0]  number of features\n    # m = n of observations\n    n = [data.shape[1]]\n    n.extend(n_nodes)\n    m = data.shape[0]\n\n    params = [None] * (L + 1)\n\n    for l in range(0, L):\n        n_next = n[l+1]\n        params[l+1] = {\n            # init formula for ReLU to avoid vanishing/exploding\n            \"W\": np.random.randn(n[l], n_next) * np.sqrt(2 / n[l]),\n            \"b\": np.zeros(n_next).reshape(n_next, 1)\n        }\n\n    return params\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef relu(x: float) -> float:\n    return np.maximum(0, x)\n\ndef sigmoid(x: float) -> float:\n    return 1.0 / (1 + np.exp(-x))\n\ndef relu_prime(x: float) -> float:\n    return (x > 0) * 1\n\ndef sigmoid_prime(x: float) -> float:\n    return sigmoid(x) * (1.0 - sigmoid(x))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef forward(data: np.ndarray, params: list[dict], activation_funs: list[str], dropout_probs: list[float]):\n    L = len(params)\n\n    cache = [None] * L\n    cache[0] = {\"A\": data}\n    A = None\n\n    for l in range(1, L):\n        Z = np.dot(cache[l - 1][\"A\"], params[l][\"W\"]) + params[l][\"b\"].T\n        A = globals()[activation_funs[l]](Z)\n        # dropout\n        D = (np.random.rand(A.shape[0], A.shape[1]) < dropout_probs[l - 1]).astype(int)\n        A = (A * D) / dropout_probs[l - 1]\n\n        cache[l] = {\n            \"W\": params[l][\"W\"],\n            \"b\": params[l][\"b\"],\n            \"Z\": Z,\n            \"A\": A,\n            \"D\": D,\n            \"fun\": activation_funs[l]\n        }\n\n    return A, cache\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef cost(y_hat, y) -> float:\n    m = y_hat.shape[0]\n    epsilon = 1e-5\n    cost = np.sum(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))/m * -1\n    return float(np.squeeze(cost))\n\ndef plot_costs(costs, learning_rate):\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('Cost')\n    plt.xlabel('Iteration')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n```\n:::\n\n\n$$\n\\begin{aligned}\nd\\mathbf{A^{[l \\ne L]}} &= d\\mathbf{Z^{[l+1]}\\mathbf{W^{[l+1]\\textsf{T}}}} \\\\\nd\\mathbf{A^{[L]}} &= \\frac{y}{\\hat{y}} -\\frac{1 - y}{1 - \\hat{y}} \\\\\nd\\mathbf{Z^{[l]}} &= d\\mathbf{A^{[l]}}\\odot g^{[l]\\prime}(Z^{[l]})\\\\\nd\\mathbf{W^{[l]}} &= \\frac{1}{m}\\mathbf{A^{[l-1]\\textsf{T}}}\\odot d\\mathbf{Z^{[l]}} \\\\\ndb^{[l]} &= \\frac{1}{m}\\sum_{i=1}^{m}dZ_{ij}^{[l]}\n\\end{aligned}\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef backward(cache: list[dict], y, dropout_probs: list[float]) -> list[dict]:\n    L = len(cache)\n    gradients = [None] * L\n    AL = cache[L - 1][\"A\"]\n    m = AL.shape[0]\n    dZ_prev = None\n    for i in range(L, 1, -1):\n        l = i - 1\n        A_prev = cache[l-1][\"A\"]\n        Z = cache[l][\"Z\"]\n        b = cache[l][\"b\"]\n        if i == L:\n            dA = - (np.divide(y, AL) - np.divide(1 - y, 1 - AL))\n            # dZ = (AL - y)\n        else:\n            W_plus1 = cache[l+1][\"W\"]        \n            dA = np.dot(dZ_plus1, W_plus1.T)\n        \n        # dropout\n        D = (np.random.rand(dA.shape[0], dA.shape[1]) < dropout_probs[l-1]).astype(int)\n        dA = (dA * D) / dropout_probs[l-1]\n\n        fun = cache[l][\"fun\"]    \n        dZ = np.multiply(dA, globals()[fun + \"_prime\"](Z))\n        dW = np.dot(A_prev.T, dZ)/m\n        db = np.sum(dZ.T, axis=1, keepdims=True)/m\n        dZ_plus1 = dZ\n        gradients[l] = {\n            \"W\": dW,\n            \"b\": db\n        }\n\n    return gradients\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef update_params(params, grads, learning_rate):\n    L = len(grads)\n\n    for l in range(1, L):\n        params[l][\"W\"] -= learning_rate * grads[l][\"W\"]\n        params[l][\"b\"] -= learning_rate * grads[l][\"b\"]\n\n    return\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef dnn_model(\n    X, Y,\n    nodes_per_layer: list[int],\n    activation_funs: list[str],\n    params: dict = None,\n    learning_rate: float = 1,\n    n_iter: int = 10000,\n    tol: float = 1e-5,\n    seed: int = None,\n    print_cost: bool = True,\n    plot_cost: bool = True,\n    dropout_probs = None\n    ) -> dict:\n    start = time.time() # start timer\n    if seed is not None:\n        np.random.seed(seed)\n    assert all([foo in globals() for foo in activation_funs])    \n    assert len(nodes_per_layer) == len(activation_funs)\n    if dropout_probs is not None:\n        assert len(nodes_per_layer) == len(dropout_probs)\n    else:\n        dropout_probs = [1] * len(nodes_per_layer)\n    n_layers = len(nodes_per_layer)\n    activation_funs.insert(0, None)\n\n    J = [1] * n_iter\n    all_learning_rates = [0] * n_iter\n    n_falling_cost = 0\n\n    if params == None:\n        params = init_params(X, n_layers, nodes_per_layer)\n    \n    Y_hat = None\n\n    for i in range(0, n_iter):\n        Y_hat, cache = forward(X, params, activation_funs, dropout_probs)\n        J[i] = cost(Y_hat, Y)\n        if (J[i] <= J[i - 1]):\n            n_falling_cost += 1\n            if n_falling_cost % 10 == 0:\n                learning_rate *= 1.1\n        else:\n            n_falling_cost = 0\n            learning_rate /= 1.1\n        if i > 0 and abs(J[i - 1] - J[i]) < tol:\n            end = time.time()\n            elapsed = end - start\n            print (f'Model converged in {round(elapsed, 1)} seconds after {i + 1} iterations')\n            break\n        gradients = backward(cache, Y, dropout_probs)\n        update_params(params, gradients, learning_rate)\n        all_learning_rates[i] = learning_rate\n\n        if print_cost:\n            if (i == (n_iter)) or ((i + 1) % 100 == 0):\n                print (f'Cost after iteration {i + 1}: {J[i]}')\n            if i > 0 and i % 1000 == 0:\n                plot_costs(J[0:i], learning_rate)\n    \n    \n    if i == (n_iter - 1):\n        if (tol == 0):\n            print(\"All iterations completed\")\n        else:\n            print(\"Model failed to converge\")\n\n    Y_pred = (Y_hat > .5).astype(int)\n    acc = float((np.dot(Y.T, Y_pred) + np.dot(1 - Y.T, 1 - Y_pred)) / float(Y.size) * 100)\n\n    if plot_cost:\n        plot_costs(J[0:i], learning_rate)\n    \n    return {\"params\": params, \"cost\": J[0:i], \"activation_funs\": activation_funs, \"yhat\": Y_hat, \"prediction\": Y_pred, \"accuracy\": acc, \"alpha\": all_learning_rates[0:i], \"dropout_probs\": dropout_probs}\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nm1 = dnn_model(\n    X = X_train,\n    Y = Y_train,\n    nodes_per_layer = [100, 20, 7, 5, 1],\n    activation_funs = [\"relu\", \"relu\", \"relu\", \"relu\", \"sigmoid\"],\n    learning_rate=0.0075,\n    n_iter=300,\n    tol=1e-10,\n    # dropout_probs = [.5, .8, .8, 1, 1]\n    dropout_probs = [1, 1, 1, 1, 1]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCost after iteration 100: 0.6117485627940126\nCost after iteration 200: 0.5417625971260696\nCost after iteration 300: 0.48608098591430077\nModel failed to converge\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Testing the model\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef predict(X, model):\n    yhat, steps = forward(\n        X,\n        model[\"params\"],\n        model[\"activation_funs\"],\n        dropout_probs = model[\"dropout_probs\"]\n    )\n    return (yhat > .5).astype(int)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npred = predict(X_test, m1)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nacc_test = float((np.dot(Y_test.T, pred) + np.dot(1 - Y_test.T, 1 - pred)) / float(Y_test.size) * 100)\n```\n:::\n\n\nOur simple model correctly classified 83.2% of the floor  plans in the training set and 70.1% of the floor  plans in the test set.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}