{
  "hash": "a76d00e7fd6f7ee7f226043c7760603d",
  "result": {
    "markdown": "---\ntitle: \"Shallow neural network exercise\"\nauthor: Milan\ndate: \"2/19/2023\"\ndate-format: \"D MMM YYYY\"\ncategories: [Neufert 4.0, machine learning]\nimage: img/nn.jpg\nexecute:\n  warning: false\n  message: false\n#   cache: true\n---\n\nLet's see if I can write a single hidden layer NN that would be able to say if an image of a floor plan shows a one-room apartment or not.\n\nFirst, load libraries.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport cv2\nimport random\nimport matplotlib.pyplot as plt\nimport time\n```\n:::\n\n\nRead in the data:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv(\"../../static/data/all_flats_clustered.csv\")\n```\n:::\n\n\n## Data processing\n\nTo make things simple, I'm going to only work with one apartment per shape cluster.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.drop_duplicates(\"cluster\", keep = \"first\", inplace = True)\n```\n:::\n\n\nI also only want to keep 2,000 one- and four-room apartments.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf = df[(df.room_number == 1) | (df.room_number == 4)]\ndf = df.head(2000)\n```\n:::\n\n\nLet's read in the floor plan images. These are saved locally on my computer, soz...  <span class=\"lol\">¯\\\\\\_(ツ)\\_/¯</span>\n\nI'll create an empty array and then append each image in a loop. `cv2.imread(x, 0)` reads the files in greyscale. I want to normalise the values so that they don't range between 0 and 255 but 0-1. I also want to get rid of the greyscale by forcing non-white pixels to be black. That is what `... == 1` does and `astype(int)` casts the resulting `boolean` as `integer`.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nall_data = []\nfor i in range(0, df.shape[0]):\n    img = cv2.imread(\"../../../similarity/0\" + str(df.room_number.iloc[i]) + \"_room/\" + str(df.room_number.iloc[i]) + \"room_\" + df.bfa.iloc[i] + \".png\", 0)\n    all_data.append(((img / 255) == 1).astype(int))\n\n# convert to NumPy array so that it has .shape\nall_data = np.array(all_data)\n```\n:::\n\n\nLet's see if that worked:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.imshow(all_data[0], cmap = \"gray\")\n```\n\n::: {.cell-output .cell-output-display execution_count=96}\n```\n<matplotlib.image.AxesImage at 0x17e9086c0a0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=424 height=415}\n:::\n:::\n\n\nNow, let's flatten the images, reshaping each to a 1D array.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nall_data = all_data.reshape(all_data.shape[0], -1)\n\ndf = df[\"room_number\"].values.reshape(df.shape[0], 1) # only keep relevant columns\ndf = (df == 1).astype(int)\n```\n:::\n\n\nAs a final step in data wrangling, I'm creating a 30-70 split of the data into train and test sets.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntrain_ind = np.random.choice(range(0, df.shape[0]), 600, replace=False)\nmask = np.zeros(df.shape[0], dtype=bool)\nmask[train_ind] = True\nX_train = all_data[mask]\nY_train = df[mask]\nX_test = all_data[~mask]\nY_test = df[~mask]\n```\n:::\n\n\n## Model design\n\n![Structure of the neural network. Stolen from Coursera.](img/model_structure.png)\n\n## Forward propagation\n\n::: {.callout-note}\nI'm following a linear regression convention below as that's the one I'm most familiar with. In ML literature, you may find the same equation expressed as $\\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]\\textsf{T}}\\mathbf{X} + b$. This is really the same thing and the difference is only due to the fact that the $\\mathbf{Z}^{[1]}$ and $\\mathbf{W}^{[1]}$ matrices here are transposed with respect to those in ML literature.\n:::\n\n$$\\mathbf{Z}^{[1]} = \\mathbf{X}\\mathbf{W}^{[1]} + B^{[1]},$$\n\n\nwhere\n\n$$\n\\mathbf{X} = \\begin{bmatrix}\n    x_{11} & \\dotsb & x_{1n}\\\\\n    x_{21} & \\dotsb & x_{2n}\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    x_{m1} & \\dotsb & x_{mn}\n\\end{bmatrix},\n$$\n\nis the $m\\times n$ matrix of $n$ features for $m$ training examples (observations).\n\n\n$$\n\\mathbf{W}^{[1]} = \\begin{bmatrix}\n    w_{11} & w_{12} & w_{13} & w_{14}\\\\\n    w_{21} & w_{22} & w_{23} & w_{24}\\\\\n    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n    w_{n1} & w_{n2} & w_{n3} & w_{n4}\n\\end{bmatrix},\n$$\n\nis a $n\\times 4$ matrix of weights,\nwith each column representing the weights for one of the four nodes of the hidden layer. Rows contain the regression weights for all of the $n$ features.\n\n$$\nB^{[1]} = \\begin{bmatrix}\n    b_1 \\\\\n    b_2 \\\\\n    b_3 \\\\\n    b_4\n\\end{bmatrix}\n$$\n\nis a column vector of biases/intercepts.\n\nThe resulting matrix $\\mathbf{Z}^{[1]}$ is a $m\\times 4$ matrix of outcomes of this linear regression with one column per node of the hidden layer.\n\n$$\n\\begin{aligned}\n\\mathbf{A}^{[1]} &= \\tanh(\\mathbf{Z}^{[1]}) \\\\\nZ^{[2]} &= \\mathbf{A}^{[1]}W^{[2]} + b^{[2]} \\\\\n\\hat{Y} &= Z^{[2]} = \\sigma(Z^{[2]}) \\\\\ny^{(i)}_{pred} &= \\begin{cases}\n    1 & \\text{if }\\hat{y}^{(i)} > 0.5 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\end{aligned}\n$$\n\nIn the above, $\\hat{Y}$ is a column vector of $m$ predicted probabilities, one for each training example. $\\hat{y}^{(i)}$ and $y^{(i)}_{pred}$ are the predicted probability and the categorical prediction for the $i$^th^ example, respectively.\n\n::: {.callout-note}\nSince this network has only 2 layers, $W^{[2]}$, $Z^{[2]}$ and $A^{[2]}$ are column vectors, hence the italics.\n:::\n\n### FP implementation\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    return (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)\n\ndef model_dimensions(data, outcome, n_nodes = 4):\n    # n = number of predictor features\n    # m = n of observations\n    # y_size = n of outcome features\n    n = data.shape[1]\n    m = data.shape[0]\n    y_size = outcome.shape[1]\n\n    return n, m, y_size, n_nodes\n\ndef params_init(n_x_features, n_y_features, n_nodes):\n    W1 = np.random.randn(n_x_features, n_nodes) * .001\n    b1 = np.zeros(n_nodes).reshape(n_nodes, 1)\n    W2 = np.random.randn(n_nodes, 1) * .001\n    b2 = 0\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\ndef forward(x, params):\n    W1 = params[\"W1\"]\n    b1 = params[\"b1\"]\n    W2 = params[\"W2\"]\n    b2 = params[\"b2\"]\n    \n    Z1 = np.dot(x, params[\"W1\"]) + params[\"b1\"].T\n    A1 = tanh(Z1)\n    Z2 = np.dot(A1, W2) + b2\n    A2 = sigmoid(Z2)\n\n    steps = {\n        \"Z1\": Z1,\n        \"A1\": A1,\n        \"Z2\": Z2,\n        \"A2\": A2\n    }\n\n    return A2, steps\n```\n:::\n\n\n## Cost function\nThe loss function of this model for a single example is:\n\n$$\n\\mathcal{L}(\\mathbf{W}^{[1]}, B^{[1]}, W^{[2]}, b^{[2]}) = y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{y}^{(i)}),\n$$\n\nwith the model cost function being the average of all $\\mathcal{L}$, one for each example:\n\n$$\n\\mathcal{J} = -\\frac{1}{2}\\sum_{i=0}^{m}\\mathcal{L}_i\n$$\n\n\n### Cost implementation\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef cost(predicted, observed):\n    yhat = predicted\n    m = yhat.shape[0]\n    y = observed\n\n    cost = -np.mean(y * np.log(yhat) + (1 - y) * np.log(1 - yhat))\n\n    return float(np.squeeze(cost))\n```\n:::\n\n\n## Back propagation\n\nGiven $\\mathcal{J}$ defined above, we need the following derivatives[^1] to implement the back propagation:\n\n[^1]: I am really not sure if the terminology and notation are technically correct. The equations below represent the individual steps for the back propagation simultaneously over all examples. The outputs of the individual steps range from scalar to matrix. Whether or not \"derivative\" is the appropriate term, I don't know. I'm just a dumdum <span class=\"lol\">¯\\\\\\_(ツ)\\_/¯</span>\n\n:::{.callout-note}\n$\\circ$ denotes element-wise product.\n:::\n\n$$\n\\begin{aligned}\n\\mathcal{J}^\\prime_{Z^{[2]}} &= A^{[2]} - Y \\\\ &= dZ^{[2]}\\\\\n\\mathcal{J}^\\prime_{W^{[2]}} &= \\frac{1}{m}\\mathbf{A}^{[1]\\textsf{T}}dZ^{[2]} \\\\\n\\mathcal{J}^\\prime_{b^{[2]}} &= \\frac{1}{m}\\sum_{i=1}^{m}dZ_i^{[2]} \\\\\n\\mathcal{J}^\\prime_{\\mathbf{Z}^{[1]}} &= dZ^{[2]}\\mathbf{W}^{[2]\\textsf{T}} \\circ g^\\prime(\\mathbf{Z}^{[1]}) \\\\\n&= dZ^{[2]}\\mathbf{W}^{[2]\\textsf{T}} \\circ (1 - \\mathbf{A}^{[1]}\\circ\\mathbf{A}^{[1]}) \\\\\n&= dZ^{[1]} \\\\\n\\mathcal{J}^\\prime_{\\mathbf{W}^{[1]}} &= \\frac{1}{m}\\mathbf{X}^\\textsf{T}dZ^{[1]} \\\\\n\\mathcal{J}^\\prime_{B^{[1]}} &= \\frac{1}{m}\\sum_{i=1}^{m}dZ_{ij}^{[1]} \\\\\n\\end{aligned}\n$$\n\n$g^\\prime(\\mathbf{Z}^{[1]})$ is the derivative of the activation (link) function in the hidden layer, in this case the $\\tanh$ function. $g(\\mathbf{Z}^{[1]}) = \\tanh(\\mathbf{Z}^{[1]}) = \\mathbf{A}^{[1]}$ and its derivative is $1 - \\mathbf{A}^{[1]}\\circ\\mathbf{A}^{[1]}$.\n\n### BP implementation\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef backward(X, Y, params, fwd_steps):\n    m = X.shape[0]\n    A1 = fwd_steps[\"A1\"]\n    A2 = fwd_steps[\"A2\"]\n    W1 = params[\"W1\"]\n    W2 = params[\"W2\"]\n    b1 = params[\"b1\"]\n    b2 = params[\"b2\"]\n\n    dZ2 = A2 - Y\n    dW2 = np.dot(A1.T, dZ2)/m\n    db2 = np.mean(dZ2)\n    dZ1 = np.dot(dZ2, W2.T) * (1 - A1 ** 2)\n    dW1 = np.dot(X.T, dZ1)/m\n    db1 = np.sum(dZ1.T, axis=1, keepdims=True)/m\n\n    # print(db1.shape)\n\n    return {\n        \"dW1\": dW1,\n        \"db1\" : db1,\n        \"dW2\": dW2,\n        \"db2\" : db2,\n    }\n```\n:::\n\n\nOn top of the back propagation function, we also need a function that updates parameters. The one below modifies the object passed to `params=` in place and doesn't return anything.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndef update_params(params, grads, learning_rate):\n    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n    params[\"b1\"] -= learning_rate * grads[\"db1\"]\n    params[\"b2\"] -= learning_rate * grads[\"db2\"]    \n\n    return\n```\n:::\n\n\n## Full model\n\nOK, time to put it all together. I'm also including a tolerance parameter. If the absolute difference between the costs of two consecutive iterations, the model is deemed to have converged and the loop breaks.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef nn_model(X, Y, n_nodes, learning_rate=.2, n_iter=10000, tol=1e-5, seed = None, print_cost=True):\n    start = time.time() # start timer\n    if seed is not None:\n        np.random.seed(seed)\n\n    J = []\n    n_x, n_samp, n_y, foo = model_dimensions(X, Y, n_nodes)\n    gradients = {\n        \"dW1\": np.zeros(n_x * n_nodes).reshape(n_x, n_nodes),\n        \"db1\" : np.zeros(n_nodes).reshape(n_nodes, 1),\n        \"dW2\": np.zeros(n_nodes).reshape(n_nodes, 1),\n        \"db2\" : 0,\n    }\n    params = params_init(n_x, n_y, n_nodes)\n\n    for i in range(0, n_iter + 1):\n        if i == n_iter:\n            print(\"Model failed to converge\")\n            break\n        Y_hat, steps = forward(X, params)\n        J.append(cost(Y_hat, Y))\n        if i > 0 and abs(J[i - 1] - J[i]) < tol:\n            end = time.time()\n            elapsed = end - start\n            print (f'Model converged in {round(elapsed, 1)} seconds after {i + 1} iterations')\n            break\n        gradients = backward(X, Y, params, steps)\n        update_params(params, gradients, learning_rate)\n\n        if print_cost and i % 50 == 0:\n            print (f'Cost after iteration {i}: {J[i]}')\n        \n\n    return {\"params\": params, \"cost\": J}\n```\n:::\n\n\nCool, all that's left to do is run the model on the training set:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nm1 = nn_model(X_train, Y_train, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCost after iteration 0: 0.6931060324765631\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nModel converged in 2.2 seconds after 18 iterations\n```\n:::\n:::\n\n\nThat was actually way faster than I would have expected...\n\n\n## Testing the model\n\nFirst, a quick wrapper function that returns a binary array of predictions: 0 = not a one-room floor plan; 1 = one-room floor plan.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef predict(X, params):\n    yhat, steps = forward(X, params)\n    return (yhat > .5).astype(int)\n```\n:::\n\n\nNext, we need to use the previously learnt parameters stored in `m1` and pass the test dataset to `predict()`.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\npred = predict(X_test, m1[\"params\"])\n```\n:::\n\n\nFinally, let's calculate the classification accuracy:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nacc = float((np.dot(Y_test.T, pred) + np.dot(1 - Y_test.T, 1 - pred)) / float(Y_test.size) * 100)\n```\n:::\n\n\n::: {.cell execution_count=18}\n\n::: {.cell-output .cell-output-display}\n\nOur simple model correctly classified 64.1% of the floor  plans in the test set\n\n:::\n:::\n\n\nNot amazing but given that the model only uses raw pixel values of the images, it's not that bad either.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}